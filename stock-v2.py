# -*- coding: utf-8 -*-
"""Stock_v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F1h3bP-J39wqgBqfnqvAKfcaYvRaWUkS

read text file
"""

import pandas as pd
import numpy as np

data=pd.read_csv('/content/AAPL.csv')
data.head(3)
d1=data.sort_values('Low')

label_d1=[]
l_one=np.ones((50,))*3
l_two=np.ones((50,))*0
l_three=np.ones((50,))*4
l_four=np.ones((50,))
l_five=np.ones((48,))*2

label_d1=label_d1+list(l_one)+list(l_two)+list(l_three)+list(l_four)+list(l_five)

import pandas as pd
data2=pd.read_csv('/content/textfile_withdate.csv')

data2.columns=['date','label','text']
data2.head(2)

"""pre-processing on text"""

import re # regex library
from keras.preprocessing.text import text_to_word_sequence, Tokenizer

def preprocessor(text):
    letters_only = re.sub("[^a-zA-Z]",  # Search for all non-letters
                          " ",          # Replace all non-letters with spaces
                          str(text))
    return letters_only
  
data2['text'] = data2['text'].apply(preprocessor)

data.shape

"""Convert text to numeric-vector"""

from gensim.models import Word2Vec
import time

size = 7
window = 3
min_count = 1
workers = 3
sg = 1

word2vec_model_file ='word2vec_' + str(size) + '.model'
start_time = time.time()
stemmed_tokens = pd.Series(data2['text']).values

w2v_model = Word2Vec(stemmed_tokens, min_count = min_count, size = size, workers = workers, window = window, sg = sg)
print("Time taken to train word2vec model: " + str(time.time() - start_time))
w2v_model.save(word2vec_model_file)

import numpy as np

sg_w2v_model = Word2Vec.load(word2vec_model_file)

word2vec_filename = 'train_word2vec.csv'
with open(word2vec_filename, 'w+') as word2vec_file:
    for i in range(data2['text'].shape[0]):
        model_vector = (np.mean([sg_w2v_model[token] for token in data2['text'][i]], axis=0).tolist())
        if i == 0:
            header = ",".join(str(ele) for ele in range(7))
            word2vec_file.write(header)
            word2vec_file.write("\n")

        if type(model_vector) is list:  
            line1 = ",".join( [str(vector_element) for vector_element in model_vector] )
        else:
            line1 = ",".join([str(0) for i in range(7)])
        word2vec_file.write(line1)
        word2vec_file.write('\n')

"""convert labels to numerics"""

label=[]

for i in range(data2['label'].shape[0]):
  if data2['label'].iloc[i]=='Buy':
    label.append(0)
  elif data2['label'].iloc[i]=='Sell':
    label.append(1)
  elif data2['label'].iloc[i]=='Strong Sell':
    label.append(2)
  elif data2['label'].iloc[i]=='Strong Buy':
    label.append(3)
  elif data2['label'].iloc[i]=='Hold':
    label.append(4)

"""to-categorical labels"""

from tensorflow.keras.utils import to_categorical

label_ca=to_categorical(label, dtype ="uint8")

label_all=label+label_d1

new_textdata=pd.read_csv('train_word2vec.csv')
new_textdata.shape

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(new_textdata,label_ca, test_size=0.33, random_state=42)

"""model and fit the model"""

from keras import backend as K

def recall_m(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
        recall = true_positives / (possible_positives + K.epsilon())
        return recall

def precision_m(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
        precision = true_positives / (predicted_positives + K.epsilon())
        return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

###model CNNlstm
import keras
import sklearn
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Embedding, Flatten
from keras.layers import LSTM, SimpleRNN, GRU,GaussianNoise, Bidirectional, BatchNormalization,Convolution1D,MaxPooling1D, Reshape, GlobalAveragePooling1D
import sklearn.preprocessing
from sklearn import metrics
from scipy.stats import zscore
from tensorflow.keras.utils import get_file, plot_model
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt

batch_size = 64
model = Sequential()
model.add(Convolution1D(64, kernel_size=64,padding='same',activation="relu",input_shape=(1,7)))
model.add(BatchNormalization())
model.add (GaussianNoise(0.2))
model.add(LSTM(64, return_sequences=False))
model.add(Dropout(0.1))
model.add(Dense(5))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=[f1_m,recall_m,precision_m])
model.summary()

X_train=np.resize(X_train,(28716,1, 7))
X_test=np.resize(X_test,(14145,1, 7))

y_test=np.array(y_test)
y_train=np.array(y_train)

history=model.fit(X_train, y_train, epochs=10,batch_size=128, validation_data=(X_test,y_test))

new_textdata=np.resize(new_textdata,(42861, 1, 7))

one_hot=model.predict(new_textdata)

del d1['Volume']
del d1['Date']

d1.shape

all_data=np.concatenate((one_hot, d1), axis=0)

all_data.shape

label_all=list(label)+list(label_d1)

len(label_all)

### model LSTM 
###model
import keras
import sklearn
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Embedding, Flatten
from keras.layers import LSTM,TimeDistributed, SimpleRNN, GRU, Bidirectional,Conv1D, BatchNormalization,Convolution1D,MaxPooling1D, Reshape, GlobalAveragePooling1D
import sklearn.preprocessing
from sklearn import metrics
from scipy.stats import zscore
from tensorflow.keras.utils import get_file, plot_model
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt


# define LSTM
model_2 = Sequential()
model_2.add(LSTM(20, input_shape=(1,5), return_sequences=True))
model_2.add(Dense(5, activation='softmax'))
model_2.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=[f1_m,recall_m,precision_m])

from tensorflow.keras.utils import to_categorical

label_ca2=to_categorical(label_all, dtype ="uint8")

label_ca2=np.resize(label_ca2,(43109, 1,5))

label_ca2.shape

all_data.shape

all_data=np.resize(all_data,(43109,1, 5))

type(all_data)

#label_all=np.array(label_all)

from sklearn.model_selection import train_test_split
X_train1, X_test1, y_train1, y_test1 = train_test_split(all_data,label_ca2, test_size=0.33, random_state=42)



history_2=model_2.fit(X_train1, y_train1, epochs=10,batch_size=128, validation_data=(X_test1,y_test1))

